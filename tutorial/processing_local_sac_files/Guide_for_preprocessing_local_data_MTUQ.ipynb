{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab2103f",
   "metadata": {},
   "source": [
    "# Manual for pre-processing local data for running MTUQ\n",
    "\n",
    "#### Félix Rodríguez-Cardozo and Jochen Braunmiller\n",
    "\n",
    "This manual is intended for reading SAC files to pre-process them for creating SAC files in the format required by [MTUQ](https://github.com/uafgeotools/mtuq) for running a moment tensor grid-search. SAC file name requirement is to end in '.sac' \n",
    "\n",
    "Before starting, let's explore the directory to become familiar with the minimun requirements to use this notebook. \n",
    "\n",
    "#### Note:  If you want to run  this notebook again clean the workspace by calling the subroutine clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c4459-a2d9-41e3-b6a1-1436a3e5fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def clean():\n",
    "    print('rm -r PROCESSED_DATA BU_PROCESSED_DATA RESP_FILES')\n",
    "    os.system('rm -r PROCESSED_DATA BU_PROCESSED_DATA RESP_FILES __pycache__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a316a5-1aa9-4c59-bdbc-53059285408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to clean the workspace run this block of code\n",
    "clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0635a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls for seeing the files in the preprocessing directory\n",
    "import os\n",
    "! ls -d *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c71fbf-c41f-4a9a-86de-23e2afe41234",
   "metadata": {},
   "source": [
    "# Directory Overview - Getting Started "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378c29d",
   "metadata": {},
   "source": [
    "### 1. **DATA** \n",
    "This directory contains the files in SAC format that will be processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b816620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls DATA/* for seeing the raw SAC files \n",
    "!ls DATA/20171201023244/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbbd54",
   "metadata": {},
   "source": [
    "For this example we provide data from 3 seismic stations in Iran. The data come from the Iranian National Seismological Network (ISNS) and the Iranian Seismological Center (IRSC). The way in which the SAC files are named is slightly different between the two data sources. Still, as long as the file names are ending in '.sac' this notebook should be able to work with any name. It is propably a good idea that file names say something about station name, channel, (network), and date-and-time of data (perhaps when they start). \n",
    "\n",
    "The directory with the SAC files for each event should follow the format: **yyyymmddhhiiss**. Where **yyyy** stands for year, **mm** for month, **dd** for day, **hh** for hour, **ii** for minute and **ss** for seconds. \n",
    "\n",
    "#### NOTE: It is important that the SAC files have a header with, at minimum information, channel and station name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the header information of one of the seismograms:\n",
    "from obspy import read\n",
    "st = read('DATA/20171201023244/171201.0232.BSRN.BHZ.sac')\n",
    "print(st[0].stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f2095",
   "metadata": {},
   "source": [
    "### 2. events_list.txt \n",
    "\n",
    "Plain text file with basic information about origin time, hypocenter, and magnitude of the to-be-pre-processed earthquake. The SAC files corresponding to the events listed in events_list.txt should be saved in the **DATA** directory following the aforementioned format.\n",
    "\n",
    "The events list has the following format:\n",
    "\n",
    "**YEAR MONTH DAY HOUR:MIN:SEC LATITUDE LONGITUDE MAGNITUDE DEPTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92893783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the events_list.txt file\n",
    "! cat events_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2c7c6",
   "metadata": {},
   "source": [
    "### 3. pzfiles \n",
    "\n",
    "Directory with the SAC pole and zero files needed to remove the instrument response. The pole-and-zero SAC file names have to follow the format **STATIONNAME_CHANNEL.pz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8af46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the content of one the poles and zeros files\n",
    "! ls pzfiles\n",
    "! cat pzfiles/BSRN_BHZ.pz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77395f11",
   "metadata": {},
   "source": [
    "#### 3. station_list.txt:\n",
    "\n",
    "Plain text file with seismic station information for stations with data to be pre-processed. This file should follow the format:\n",
    "\n",
    "**STATIONNAME LATITUDE LONGITUDE ELEVATION_IN_METERS NETWORK**\n",
    "\n",
    "Note: Elevation is not used further but an entry in this column is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the station_list.txt content\n",
    "! cat station_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a154",
   "metadata": {},
   "source": [
    "### 4. run_MTUQ: \n",
    "\n",
    "Once the seismograms are preprocessed, this directory contains an example for running MTUQ. We will see details of that directory and how to run MTUQ later.\n",
    "\n",
    "### 5. local_preprocessing_mtuq.py:\n",
    "\n",
    "Python script that is the module for preprocessing the SAC files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99edac",
   "metadata": {},
   "source": [
    "# Preprocessing SAC files\n",
    "\n",
    "To use this notebook it is important to have already loaded the MTUQ module. In addition, it is important to load the following libraries including the one designed for preprocessing the local data (**local_preprocessing_mtuq.py**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading libraries to use in this notebook\n",
    "import local_preprocessing_mtuq\n",
    "import importlib\n",
    "importlib.reload(local_preprocessing_mtuq)\n",
    "from obspy import read\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from obspy import Trace\n",
    "from obspy.io.sac import sacpz\n",
    "from obspy import read, read_inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdc1ad",
   "metadata": {},
   "source": [
    "Run the follow block of code. It is a function for plotting seismograms and we will use it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for comparing two seismograms\n",
    "def plot_event(tr1,tr2,zoom):\n",
    "    \n",
    "    lim1 = round(len(tr1.times(\"matplotlib\"))*zoom[0])\n",
    "    lim2 = round(len(tr1.times(\"matplotlib\"))*zoom[1])\n",
    "    \n",
    "    x1 = tr1.times(\"matplotlib\")[lim1:len(tr1.times(\"matplotlib\"))-lim2]\n",
    "    y1 = tr1.data[lim1:len(tr1.times(\"matplotlib\"))-lim2]\n",
    "    \n",
    "    x2 = tr2.times(\"matplotlib\")[lim1:len(tr1.times(\"matplotlib\"))-lim2]\n",
    "    y2 = tr2.data[lim1:len(tr1.times(\"matplotlib\"))-lim2]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.plot(x1, y1, \"b-\", label='Raw data')\n",
    "    ax.plot(x2, y2, \"r-\", label='Rotated data')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.xaxis_date()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e7597",
   "metadata": {},
   "source": [
    "### 1. Reading event list:\n",
    "The first step is to create a list of events to be pre-processed. This task is performed by using the attribute *add_list_events*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list='events_list.txt'\n",
    "ev_file_list=local_preprocessing_mtuq.add_list_events(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4abff5",
   "metadata": {},
   "source": [
    "The object *ev_file_list* is a list of objects of the class *Earthquake*. Each element of the list (i.e., an object of the class Earthquake) contains the information provided in the file *events_list.txt*. That information can be accessed via the objects using the attributes: \n",
    "\n",
    "- or_time: event origin time.\n",
    "- lat: event latitude.\n",
    "- lon: longitude.\n",
    "- depth: depth.\n",
    "- m: magnitude.\n",
    "- directory: directory where the waveforms are located.\n",
    "- traces: seismograms recorded for this event.\n",
    "\n",
    "At this point, the traces and directory attributes are empty. They will be filled later. \n",
    "\n",
    "Let's access one of the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdba205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing the attribute or_time\n",
    "for ev in ev_file_list:\n",
    "    print(ev.or_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0f0e4",
   "metadata": {},
   "source": [
    "### 2. Reading the DATA directory:\n",
    "In this step the script will search the **DATA** directory for sub-directories for each event to preprocess. The idea is to contrast the data stored in **DATA** with the information gathered in add_list_events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dir = 'DATA'\n",
    "ev_dir_list = local_preprocessing_mtuq.add_directory_events(events_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c1fbc",
   "metadata": {},
   "source": [
    "Similarly to *ev_file_list*, this previous step created the object *events_dir*. This is a list that collects objects of the class *Dir_earthquake* whose only attributes are:\n",
    "\n",
    "- or_time: origin time.\n",
    "- directory: names of the event directories found in DATA.\n",
    "\n",
    "For instance, the directory attribute for each element of the list is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the attribute directory\n",
    "for ev_dir in ev_dir_list:\n",
    "    print(ev_dir.directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9087c",
   "metadata": {},
   "source": [
    "In this case, we have a unique object unlike for the *ev_file_list* list. This is because in the **DATA** directory there are data for only one earthquake.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3c45b",
   "metadata": {},
   "source": [
    "###  3. Compare two lists:\n",
    "\n",
    "In this step we check *ev_file_list* list against *ev_dir_list* to create a unique list of the class *Earthquake* that only contains events with data in the **DATA** directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dir = 'DATA'\n",
    "joint_event_list=local_preprocessing_mtuq.merge_lists(ev_file_list,ev_dir_list,events_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c72cba",
   "metadata": {},
   "source": [
    "The new list *joint_event_list* is similar to *ev_file_list*. However, the objects of the class *Earthquake* in the list *joint_event_list* contain information in the attributes *directory* and *traces* (unlike the empty values in *ev_file_list*).\n",
    "\n",
    "You can see both attributes by running the following block code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab15f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See traces for each directory in the joint_event_list list \n",
    "for ev in joint_event_list:\n",
    "    print (ev.directory)\n",
    "    \n",
    "    for tr in ev.traces:\n",
    "        print('\\t {}'.format(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcede950",
   "metadata": {},
   "source": [
    "###  4. Quality control:\n",
    "\n",
    "This step filters out traces that do not fullfil the following requirements:\n",
    "\n",
    "- **More than 3 traces per station, channel, and location:** this requirement can be fixed by merging multiple SAC traces for a single station. Ideally, there must be at maximum three traces per station one for each Z, N, and E. \n",
    "\n",
    "- **Only one horizontal component:** if only a single horizontal component, either east or north is found, it will be rejected, since one step of the processing consists of rotating to radial and transverse components, \n",
    "\n",
    "- **Data start after the origin time:** the trace must start before or at the origin time. In addition, the end time of the trace must be after the origin time. \n",
    "\n",
    "- **Existence of a pole and zero SAC file:** each trace must have its corresponding pole an zero file in the pzfiles directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dir = 'DATA'\n",
    "filter_list_events = local_preprocessing_mtuq.quality_control(joint_event_list,events_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb2ea8",
   "metadata": {},
   "source": [
    "Since all traces passed the quality control *filter_list_events* list is identical to *joint_event_list*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ac9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See traces that passed the quality control test \n",
    "for ev in filter_list_events:\n",
    "    print (ev.directory)\n",
    "    \n",
    "    for tr in ev.traces:\n",
    "        print('\\t {}'.format(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe50f2",
   "metadata": {},
   "source": [
    "### 5. First pre-processing:\n",
    "\n",
    "With the traces that passed the quality control the following processing steps are applied:\n",
    "\n",
    "- **5.1 Cutting traces and zero padding (if necessary):** the traces are cut to start at the origin time. If traces for a single station have different lengths, then shorter traces are padded with zeros to make all traces have the same lenght.\n",
    "\n",
    "- **5.2 Detrend traces:** for each trace, [detrend is applied](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.detrend.html) twice for removing the mean and the linear trend. \n",
    "\n",
    "- **5.3 Correcting and completing SAC headers:** for each trace, the [SAC header values](http://www.adc1.iris.edu/files/sac-manual/manual/file_format.html) *depmin, depmax, depmen, evla, evlo, evdp, lovrok (TRUE), lcalda (TRUE),* and *khole* are updated. If the *khole* header value does not exist in the raw trace, then that variable is set in the header as an empty character. In addition, *OMARKER* is created in the SAC header for marking the origin time in the trace. \n",
    "\n",
    "- **5.4 Saving pre-processed seismogram in a new location:** the new pre-processed traces with the name format requested by MTUQ are saved in the directory **PROCESSED_DATA** with the same structure as in the **DATA** directory. In this step the *stla*, *stlo* and *knetwk* header values are added via reading the *station_list.txt* file. If the station location is not available in *station_list.txt*, then that trace will not be considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164ec72-15fb-4d51-acce-fbbf17dde59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launching the first pre-processing\n",
    "processed_dir='PROCESSED_DATA'\n",
    "local_preprocessing_mtuq.preprocessing(joint_event_list,processed_dir,events_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95bf2b-c0c5-455b-9409-c1af0800d53b",
   "metadata": {},
   "source": [
    "The new SAC files are in the directory **PROCESSED_DATA/20171201023244**. Further pre-processing will be applied to  these SAC files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8923391-2157-430a-8a13-9ce16d5d336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the files in the new directory\n",
    "! ls PROCESSED_DATA/20171201023244/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50477a-ff11-414c-a166-303054b8f649",
   "metadata": {},
   "source": [
    "Also observe that the header values for the new SAC files are more complete compared to the raw SAC files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e834d2-2403-4073-b9f8-b83f2bcd5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the header of one of the new SAC files\n",
    "st = read('PROCESSED_DATA/20171201023244/20171201023244.IN.BSRN..BH.z')\n",
    "print(st[0].stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a9313-d00a-44b2-b818-8fdbb3845eb9",
   "metadata": {},
   "source": [
    "### 6. Correcting for sensor mis-orientation: \n",
    "\n",
    "In some cases, seismometers are not oriented to geographic North and hence the horizontal components of the seismograms carry a systematic error. This may cause issues when deviations are large and therefore, a rotation has to be applied to correct the data for sensor misorientation. In the case for Iran, [Braunmiller et al., 2020](https://pubs.geoscienceworld.org/ssa/srl/article/91/3/1660/583389/Sensor-Orientation-of-Iranian-Broadband-Seismic) found those deviations based on P-wave particule motion analysis. \n",
    "\n",
    "In this section, a corrective rotation is applied to the horizontal traces (east and north components) if the sensor is  misorientated. The deviation (if exist) is read from the *AZIMUTH* comment in the SAC poles and zero files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f0001-3b10-4ef6-9d23-9cd39dba6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotating to the true north if it is neccesary \n",
    "#First, a  security copy just in case and for educational purposes for comparing the traces before and after the rotation.\n",
    "\n",
    "processed_dir='PROCESSED_DATA'\n",
    "! cp -r PROCESSED_DATA BU_PROCESSED_DATA\n",
    "local_preprocessing_mtuq.rotate_true_north(processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060c32a-4170-4708-91fa-717f185aebc7",
   "metadata": {},
   "source": [
    "In this example, the three stations have a deviation from true north: TNSJ (23°), BSRN (-3°) and NHDN (19°). Therefore, all horizontal traces were rotated and the SAC files overwritten. \n",
    "\n",
    "Now we are going to use the plot function defined earlier in this notebook for comparing the raw waveforms and the rotated ones.\n",
    "\n",
    "#### NOTE: the raw unrotated data is backed-up in BU_PROCESSED_DATA. This backup could be useful in case that the rotation is applied more than once by mistake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd24bf-a653-44d8-8cce-a0765f41014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let´s compare waveforms\n",
    "st = read('PROCESSED_DATA/20171201023244/20171201023244.IN.BSRN..BH.e')\n",
    "st += read('BU_PROCESSED_DATA/20171201023244/20171201023244.IN.BSRN..BH.e')\n",
    "#This zoom paremeter cut a percentage (0 to 1)from the begining and the end of the trace. \n",
    "#In this case we omit the 20% of the onset and 90% from the end.\n",
    "zoom = [0.02,0.9]\n",
    "plot_event(st[0],st[1],zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5ff31-35b5-44ba-9b77-202e495e752a",
   "metadata": {},
   "source": [
    "Station BSRN has a tiny deviation from true North (-3°), so it is expected that the seismogram after the rotation is almost identical to the unrotated one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52eb43e-a0c0-466a-8624-f93e0dd67dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the process for a station with a larger deviation:\n",
    "st = read('PROCESSED_DATA/20171201023244/20171201023244.IR.TNSJ..BH.n')\n",
    "st += read('BU_PROCESSED_DATA/20171201023244/20171201023244.IR.TNSJ..BH.n')\n",
    "zoom = [0.10,0.6]\n",
    "plot_event(st[0],st[1],zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8e1e8-b48a-4d3b-b537-6178efcd4878",
   "metadata": {},
   "source": [
    "Station TNSJ has a large deviation (23°) and the difference in the amplitudes between the rotated and unrotated data is consistent with this. A similar result can be found for NHDN (19°):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e693c08-be95-4fcf-a606-d919d503d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = read('PROCESSED_DATA/20171201023244/20171201023244.IR.NHDN..BH.n')\n",
    "st += read('BU_PROCESSED_DATA/20171201023244/20171201023244.IR.NHDN..BH.n')\n",
    "zoom = [0.13,0.5]\n",
    "plot_event(st[0],st[1],zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd423c1-b131-4084-a6ef-9b5dd442e4fc",
   "metadata": {},
   "source": [
    "#### NOTE: An important sanity check is to confirm that the vertical components remain unmodified since the rotation has to be applied only to the horizontal ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d0d7a-5ae7-4b0f-9a61-512275bcf424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check: vertical components should be identical\n",
    "st = read('PROCESSED_DATA/20171201023244/20171201023244.IR.NHDN..BH.z')\n",
    "st += read('BU_PROCESSED_DATA/20171201023244/20171201023244.IR.NHDN..BH.z')\n",
    "zoom = [0.13,0.5]\n",
    "plot_event(st[0],st[1],zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc65fe9-3003-4208-9761-fe46eb5ba536",
   "metadata": {},
   "source": [
    "### 7. Removing the instrument response:\n",
    "\n",
    "#### 7.1 Making RESP files:\n",
    "For [removing the instrument response in ObsPy](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.remove_response.html) it is necessary to create an inventory object using the [read_inventory](https://docs.obspy.org/packages/autogen/obspy.core.inventory.inventory.read_inventory.html) method. However, the SAC pole and zero files are not included among the options of input parameters of such method. The closest SAC format is the [RESP file](https://ds.iris.edu/ds/nodes/dmc/data/formats/resp/) format. Therefore, in the next step we read the pole and zero SAC files to create RESP files for the sole purpose of removing the instrument response. \n",
    "\n",
    "#### NOTE: \n",
    "Because of their nature, RESP files are more comprenhensive than the pole and zero SAC files. Therefore, if RESP files are available a better approach is to use those RESP files to [create pole and zero files](https://docs.obspy.org/packages/autogen/obspy.io.sac.sacpz._write_sacpz.html). The opposite procedure (like in this example) creates RESP files that contain some dummy values. The RESP files created in this step are only useful for removing the instrument response if the poles, zeroes, and constant information is correct; in this approach only the seismic sensor response is removed but any digitizer related system response will not be removed.\n",
    "\n",
    "For example, in the dummy RESP file the *A0 normalization factor* is filled with the *CONSTANT* value taken from the SAC pole and zero file. This is innacurate since the *CONSTANT* is equal to the product of the *A0 normalization factor*, and the *Sensitivity* (which is obtained by the product of the different gain values) in the RESP file. In this (unelegant but functional) approach, the *Sensitivity* and the gains in the RESP file are defined as 1.0 so the final product between the *A0 normalization factor* and *Sensitivity* is equal to the *CONSTANT*. \n",
    "\n",
    "In case that you do not have full access to sensor information for creating a RESP file or you suspect incorrect information in existing RESP files asides from poles, zeroes and constant, then the method presented here is a viable option.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d06e0d-376a-47ab-a35c-1a611b6ca7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the resp files for removing the instrumental response\n",
    "processed_dir = 'PROCESSED_DATA'\n",
    "local_preprocessing_mtuq.paz2resp(processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b73232-2e68-4f21-9c85-8e29a5ad80e0",
   "metadata": {},
   "source": [
    "The RESP files are createad in a directory named **RESP_FILES**. Be aware those files are only valid for removing the sensor (instrument) response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7d1a-4c9d-497d-918a-aff60717337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check one of the RESP files:\n",
    "! cat RESP_FILES/RESP.IR.NHDN..BHZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb399f5-922f-4d86-9353-40750c1ed21d",
   "metadata": {},
   "source": [
    "#### 7.2 Use the dummy RESP files for removing the instrument response:\n",
    "\n",
    "The next block of code uses the RESP files and removes the instrumental response for the SAC files saved in PROCESSED_DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d9303-aa50-4765-a58a-f869046842b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the instrumental response for all the files stored in PROCESSED_DATA\n",
    "local_preprocessing_mtuq.remove_instrumental_response_dir(processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd024b3-1636-4f98-bdb5-fa811c43af30",
   "metadata": {},
   "source": [
    "### 8. Rotate to the radial and transverse:\n",
    "\n",
    "In this step  are created the radial and transverse components to be used in MTUQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc74bc-c929-41c3-bc4d-a9a9baf2d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotating to the Radial and Transverse\n",
    "processed_dir = 'PROCESSED_DATA/'\n",
    "local_preprocessing_mtuq.rotate_radial_transverse_dir(processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11970aa2-1f6a-43f3-82ef-478ba057f1c5",
   "metadata": {},
   "source": [
    "Now, each station must have 5 traces: vertical, East, North, radial and transverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96658452-57a9-4226-a1e0-dea53b653d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the new traces:\n",
    "! ls PROCESSED_DATA/20171201023244/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8fc45-7902-4ae7-87d1-814ffbaa4321",
   "metadata": {},
   "source": [
    "### 9. Padding zeros:\n",
    "\n",
    "In some cases for MTUQ, depending on the maximum time-shifts allowed and the lenght of the seismogram, there might not be enough samples before the origin time and zeros should be added. You can use either the following block of code or the function included in MTUQ to do this.\n",
    "\n",
    "For this example, 60 s of zeros will be added.\n",
    "\n",
    "#### NOTE: the following method operates on single event directories rather than on all  events in PROCESSED_DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f6ba1-61c7-4414-8fc9-8f27966edc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padd zeros\n",
    "processed_dir = 'PROCESSED_DATA'\n",
    "event = '{}/20171201023244'.format(processed_dir)\n",
    "time = 60\n",
    "local_preprocessing_mtuq.padd_zeros(event,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b8518-8773-4068-a9c0-b66f0c57f4f2",
   "metadata": {},
   "source": [
    "### 10. Scaling the amplitudes:\n",
    "The unit of the seismogram after removing the instrument response is m/s. However, we use MTUQ in centimeters (cm) for consistency with the [MTUQ seismic moment units](https://github.com/uafgeotools/mtuq/blob/260b827fe8f4934986efd6172b9cc45eecc34318/mtuq/greens_tensor/FK.py#L19) for the Green's Functions. \n",
    "\n",
    "Therefore, in this example, the following block of code multiplies the seismogram by 100 to convert from m/s to cm/s. As for the previous step, this method operates on single event directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbbbb7-9182-4c81-bf47-43ab214e47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Amplitude\n",
    "processed_dir = 'PROCESSED_DATA'\n",
    "event = '{}/20171201023244'.format(processed_dir)\n",
    "scale = 100\n",
    "local_preprocessing_mtuq.scale_amplitude(event,scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb70a77-62cd-41ab-b663-cc4248ce0aad",
   "metadata": {},
   "source": [
    "### 11. Making weights.dat files:\n",
    "\n",
    "The last step before running MTUQ is to make the parameter input file read by MTUQ. \n",
    "The user must define whether body and/or surface waves will be included and which components will be used. \n",
    "This is done by defining the components object in the following block of code. \n",
    "\n",
    "For example, if both body (vertical and radial) and surface (vertical, radial and transverse) waves with all  components will be included for the moment tensor estimation, then the components object has to be set up as:\n",
    "\n",
    "components = '1   1   1   1   1'\n",
    "\n",
    "If only the vertical component for body waves and transverse and radial components of the surface waves are going to be used, the components object will look like:\n",
    "\n",
    "components = '1   0   1   0   1'\n",
    "\n",
    "#### NOTE: The configuration of the *components* object will be applied to all events in *PROCESSED_DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ded0f-8c4e-42d2-a27a-c6f08aaaea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writting weights.dat files for using all componentes of body and surface waves\n",
    "processed_dir = 'PROCESSED_DATA'\n",
    "components = '1 1 1 1 1'\n",
    "local_preprocessing_mtuq.write_weight_all(processed_dir,components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d16fdd-02f3-48f5-bc0c-eb7e4536ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the weights.dat file in 20171201023244\n",
    "! cat PROCESSED_DATA/20171201023244/weights.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab059228-896b-4de8-8895-66ab1f8c8872",
   "metadata": {},
   "source": [
    "### Now you are ready to run MTUQ with your local DATA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84649f31-7967-4df3-a154-bfabdd6e7c1a",
   "metadata": {},
   "source": [
    "### 12. Running the MTUQ example:\n",
    "\n",
    "In the directory **run_MTUQ** you will find a python script for running a double couple constrained moment tensor estimation in MTUQ (*FK_GFs_GridSearch_DoubleCouple_options.py*). For this example, we also provide the Green's Functions pre-calculated with the FK-method for a regional 1D velocity model in Iran for 6 km source depth (stored in directory *greens/ir/ir_6*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247b1e9-e2ac-48e2-bb02-6f50a6448b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this code of block once. Is for backing-up the main directory and going back to it when neccesary\n",
    "import os\n",
    "main_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a8429-c98e-498e-a4ab-8568f795cbc0",
   "metadata": {},
   "source": [
    "The first step is to copy the Pre-processed seismograms to the *run_MTUQ* directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08cbc53-dd3d-4de1-ab40-d5687af81a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r PROCESSED_DATA/20171201023244 run_MTUQ/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41d9c7-a50a-41a7-a4a0-770848c949ac",
   "metadata": {},
   "source": [
    "Second, take a look to the Green Functions to use for calculating the synthetic seismograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e437c1f-7e38-45b0-a8f8-20dafde8d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check the Green Functions (GF's)\n",
    "! ls run_MTUQ/greens/ir/ir_6/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8bb2c-2c0b-46b6-9de1-9b0404f224a5",
   "metadata": {},
   "source": [
    "The Green's Functions provided for this example are calculated for a source at 6km depth and distances of 216, 264, and 366 km between the epicenter and the stations. \n",
    "\n",
    "#### NOTE: \n",
    "The distances between epicenter and stations are integer values in MTUQ. Therefore MTUQ uses the numpy ceil method for rounding distances, which in some cases may create [conflicts](https://github.com/uafgeotools/mtuq/issues/173) for matching the distance calculated from the SAC files and the corresponding set of Green's Functions. This is because the round method could be used for making the weights.dat files. A patch to circumvent discrepancies in both methods for rounding the distances is to calculate Green's Functions for a distance (d) and two more distances (d+1, d-1). For example, for station TNSJ the correspondent Green's Functions are 366.grn.* but we also provided the Green's Functions 365.grn.* and 367.grn.*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7485367-d327-44a6-aae5-8c812048924d",
   "metadata": {},
   "source": [
    "Now, move to the **run_MTUQ** directory for running the *FK_GFs_GridSearch_DoubleCouple_options.py* script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbca64-2761-4544-ad5c-222c1f9ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main_dir)\n",
    "os.chdir('run_MTUQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed11f66-cd03-4ae7-b6b7-654081c0fce6",
   "metadata": {},
   "source": [
    "#### 12.1 Launching the MTUQ grid-search:\n",
    "The *FK_GFs_GridSearch_DoubleCouple_options.py* script performs a double couple grid-search. Once the module is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada01ad2-4cf0-466f-9d69-ecce2976b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the module FK_GFs_GridSearch_DoubleCouple_options\n",
    "import FK_GFs_GridSearch_DoubleCouple_options\n",
    "import importlib\n",
    "importlib.reload(FK_GFs_GridSearch_DoubleCouple_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375cca6-efeb-46b3-bb04-e8679fe575a8",
   "metadata": {},
   "source": [
    "It is possible to use the method launch_gs for starting the grid-search. Use the doscstring method to find out about the input options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa8009-5762-4f94-abd2-a99ca1d33ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (FK_GFs_GridSearch_DoubleCouple_options.launch_gs.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642adf5-7d05-41c1-8f65-f55a8f2bd86c",
   "metadata": {},
   "source": [
    "In the following block of code, the method launch_gs is called using the input parameters for the event and the corresponding pre-processed waveforms for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3b653-9d0c-41c6-af43-cde03e72cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "FK_GFs_GridSearch_DoubleCouple_options.launch_gs('20171201023244',30.734,57.39,6000.0,6.0,'2017-12-01T02:32:44.000000Z',30,'3-15','15-33',25,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17883ebf",
   "metadata": {},
   "source": [
    "The final step is to open the figure *20171201023244DC_waveforms.png* that summarizes the result of the moment tensor estimation - in this case we had constrained the source to be DC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f5789-25a0-44e7-8988-caca27477d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='20171201023244DC_waveforms.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e6161",
   "metadata": {},
   "source": [
    "# The END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a2a67-d5b5-4a8e-91fc-27afd807fa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtuq",
   "language": "python",
   "name": "mtuq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
